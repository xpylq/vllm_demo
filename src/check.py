# check_gpu.py - æ£€æŸ¥GPUç¯å¢ƒ
import torch
import subprocess


def check_gpu_environment():
    """æ£€æŸ¥GPUç¯å¢ƒæ˜¯å¦æ»¡è¶³vLLMè¦æ±‚"""

    print("=" * 60)
    print("GPUç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)

    # 1. æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼ŒvLLMéœ€è¦NVIDIA GPU")
        return False

    print("âœ… CUDAå¯ç”¨")

    # 2. æ£€æŸ¥CUDAç‰ˆæœ¬
    cuda_version = torch.version.cuda
    print(f"ğŸ“Œ CUDAç‰ˆæœ¬: {cuda_version}")

    if float(cuda_version.split('.')[0]) < 11:
        print("âš ï¸  è­¦å‘Š: CUDAç‰ˆæœ¬è¿‡ä½ï¼Œå»ºè®®11.8+")

    # 3. æ£€æŸ¥GPUä¿¡æ¯
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ“Œ GPUæ•°é‡: {gpu_count}")

    for i in range(gpu_count):
        props = torch.cuda.get_device_properties(i)
        total_memory_gb = props.total_memory / 1024 ** 3

        print(f"\nğŸ® GPU {i}: {props.name}")
        print(f"   æ˜¾å­˜: {total_memory_gb:.1f} GB")
        print(f"   è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")

        # æ£€æŸ¥è®¡ç®—èƒ½åŠ›ï¼ˆå»ºè®®7.0+ï¼Œå³V100åŠä»¥ä¸Šï¼‰
        compute_capability = float(f"{props.major}.{props.minor}")
        if compute_capability < 7.0:
            print(f"   âš ï¸  è®¡ç®—èƒ½åŠ›è¾ƒä½ï¼Œå»ºè®®ä½¿ç”¨7.0+çš„GPU")
        else:
            print(f"   âœ… è®¡ç®—èƒ½åŠ›æ»¡è¶³è¦æ±‚")

    # 4. æ£€æŸ¥nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        print("\n" + "=" * 60)
        print("nvidia-smi è¾“å‡º:")
        print("=" * 60)
        print(result.stdout)
    except FileNotFoundError:
        print("âš ï¸  nvidia-smiæœªæ‰¾åˆ°")

    return True


if __name__ == "__main__":
    check_gpu_environment()